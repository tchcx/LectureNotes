# main.py
"""
Main orchestration module to spider websites, download images, 
and report on EXIF metadata.
"""
import argparse
import utils
import scraper
import image_processor
import reporter

def main():
    """Main function to run the script."""
    parser = argparse.ArgumentParser(
        description="A tool to spider websites, download images, and report on EXIF metadata."
    )

    # --- Argument Groups ---
    input_group = parser.add_argument_group(title="Inputs", description="Provide a URL source.")
    execution_group = parser.add_argument_group(title="Behavior", description="Control scraper operation.")
    output_group = parser.add_argument_group(title="Outputs", description="Define report generation.")

    # --- Input Arguments ---
    source_group = input_group.add_mutually_exclusive_group(required=True)
    source_group.add_argument("-w", "--webpage", type=str, nargs="+", help="One or more webpage URLs.")
    source_group.add_argument("-i", "--infile", type=str, help="Path to a file containing URLs, one per line.")

    # --- Behavior Arguments ---
    execution_group.add_argument(
        "-r", "--recurse", type=int, default=2, help="Levels of recursion for the spider. Default is 2."
    )
    
    # --- Output Arguments ---
    output_group.add_argument(
        "--html", type=str, default="report.html", help="Filename for the detailed HTML report. Default is 'report.html'."
    )
    output_group.add_argument(
        "--summary-map", type=str, default="summary_map.html", help="Filename for the summary map. Default is 'summary_map.html'."
    )
    
    args = parser.parse_args()

    # --- URL Processing ---
    if args.infile:
        raw_urls = utils.read_urls_from_file(args.infile)
        target_urls = utils.validate_and_clean_urls(raw_urls)
    else: # args.webpage must be present due to required=True
        target_urls = utils.validate_and_clean_urls(args.webpage)

    if not target_urls["valid"]:
        print("üî¥ No valid URLs to process. Exiting.")
        return

    # --- Main Logic ---
    all_downloaded_files = []
    visited_urls = set()

    # 1. Spider URLs and download all images
    for url in target_urls["valid"]:
        if scraper.normalize_url(url) not in visited_urls:
            print(f"üï∑Ô∏è Starting spider at: {url}")
            all_downloaded_files.extend(
                scraper.spider_url(url, args.recurse, visited_urls)
            )

    if not all_downloaded_files:
        print("üü° No images were downloaded. Exiting.")
        return

    # 2. Process images to extract metadata
    processed_images = image_processor.process_images(all_downloaded_files)

    # 3. Create the final HTML reports if any images had geotags
    geotagged_images = [img for img in processed_images if "coords" in img]
    if geotagged_images:
        reporter.create_html_report(
            geotagged_images,
            report_filename=args.html,
            summary_map_filename=args.summary_map
        )
    else:
        print("üü° No geotagged images found to generate a report.")


if __name__ == "__main__":
    main()
