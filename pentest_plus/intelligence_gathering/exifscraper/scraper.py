# scraper.py
import requests
import os
import hashlib
import mimetypes
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlunparse
from datetime import datetime

# Add common image mime types if they are missing from the system's defaults
mimetypes.add_type("image/jpeg", ".jpg")
mimetypes.add_type("image/webp", ".webp")

def normalize_url(url: str) -> str:
    """Removes query params and fragments to treat URLs like 'a.com/p?ref=1' as 'a.com/p'."""
    parts = urlparse(url)
    return urlunparse((parts.scheme, parts.netloc, parts.path, "", "", ""))

def spider_url(base_url: str, depth: int, visited_urls: set, download_folder: str = "images") -> list:
    """Recursively spiders a website, downloading images along the way."""
    normalized = normalize_url(base_url)
    if depth < 0 or normalized in visited_urls:
        return []

    print(f"âž¡ï¸ Visiting [Depth: {depth}]: {base_url}")
    visited_urls.add(normalized)

    try:
        response = requests.get(base_url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
    except requests.RequestException as e:
        print(f"ðŸ”´ Could not fetch {base_url}: {e}")
        return []

    # Download images from the current page
    downloaded = download_images_from_soup(soup, base_url, Path(download_folder))

    # Find links and recurse
    if depth > 0:
        for link in soup.find_all("a", href=True):
            absolute_link = urljoin(base_url, link["href"])
            # Only follow links on the same domain to avoid crawling the entire web
            if urlparse(absolute_link).netloc == urlparse(base_url).netloc:
                downloaded.extend(
                    spider_url(absolute_link, depth - 1, visited_urls, download_folder)
                )
    return downloaded

def download_images_from_soup(soup: BeautifulSoup, page_url: str, download_dir: Path) -> list:
    """Finds and downloads all images from a BeautifulSoup object."""
    download_dir.mkdir(parents=True, exist_ok=True)
    img_elements = soup.find_all("img")
    downloaded_images = []

    with requests.Session() as sess:
        for img_tag in img_elements:
            img_url = img_tag.get("src")
            if not img_url:
                continue

            img_url = urljoin(page_url, img_url)

            try:
                r = sess.get(img_url, stream=True, timeout=10)
                r.raise_for_status()

                # Infer file extension from Content-Type header; fallback to URL
                content_type = r.headers.get("Content-Type")
                extension = mimetypes.guess_extension(content_type) if content_type else Path(urlparse(img_url).path).suffix
                if not extension or "html" in extension:  # Handle bad Content-Type
                    extension = ".jpg"

                # Use a URL hash for a unique filename to prevent collisions
                fname_hash = hashlib.md5(img_url.encode()).hexdigest()
                local_path = download_dir / f"{fname_hash}{extension}"

                if local_path.exists():
                    continue

                print(f"  Downloading: {img_url}")
                file_hash = hashlib.sha256()
                with open(local_path, "wb") as f:
                    for chunk in r.iter_content(8192):
                        f.write(chunk)
                        file_hash.update(chunk)

                downloaded_images.append({
                    "path": local_path,
                    "sha256sum": file_hash.hexdigest(),
                    "source_url": img_url,
                    "headers": dict(r.headers),
                    "request_time": datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC"),
                })
            except requests.RequestException as e:
                print(f"  ðŸ”´ Error downloading {img_url}: {e}")

    return downloaded_images
